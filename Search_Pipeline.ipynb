{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain sentence-transformers torch transformers pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4StzHO38NY",
        "outputId": "e3397c0d-b8fb-41c4-d831-308ed09219e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dnspython, pymongo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dnspython-2.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pymongo-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface langchain_mongodb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey7LshiV5z3R",
        "outputId": "7848c900-2a78-4f96-fb23-85c67e61f9db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "Collecting langchain_mongodb\n",
            "  Downloading langchain_mongodb-0.6.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.60)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.52.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.31.4)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.8)\n",
            "Requirement already satisfied: langchain>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.25)\n",
            "Collecting lark<2.0.0,>=1.1.9 (from langchain_mongodb)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (2.0.2)\n",
            "Requirement already satisfied: pymongo>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo>=4.6.1->langchain_mongodb) (2.7.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3->langchain_mongodb) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.3.1)\n",
            "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_mongodb-0.6.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, langchain_huggingface, langchain_mongodb\n",
            "Successfully installed langchain_huggingface-0.2.0 langchain_mongodb-0.6.2 lark-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK9yaJZY3yD1",
        "outputId": "ded31f06-a6af-4eb2-dd52-a670f593fe67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: document_routing\n",
            "Arguments: {\n",
            "  \"document_type\": \"quy_dinh\"\n",
            "}\n",
            "\n",
            "Searching for 'Sinh viên có anh chị em ruột đang theo học được giảm bao nhiêu học phí ?' in document type: quy_dinh\n",
            "Total documents of this type: 363\n",
            "Found 29 relevant documents\n",
            "============================================================\n",
            "\n",
            "📄 Result 1\n",
            "Title: Mục 8\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 1.0000\n",
            "Vector Score: 1.0000\n",
            "Text Score: 1.0000\n",
            "Content Preview: Mục 8: Sinh viên có anh chị em ruột hiện đang học tại trường Đại học Quốc tế (áp dụng kể từ người thứ hai).\n",
            "Mức miễn giảm: 10%\n",
            "Yêu cầu: Bản sao công chứng hộ khẩu thường trú hoặc bản sao công chứng gi...\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 Result 2\n",
            "Title: Mục 7\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.5792\n",
            "Vector Score: 0.2349\n",
            "Text Score: 0.9236\n",
            "Content Preview: Mục 7: Sinh viên có anh chị em ruột hiện đang học tại trường Đại học Quốc tế (áp dụng kể từ người thứ hai), (Chính sách riêng của trường ĐHOT). >\n",
            "Mức miễn giảm: 10% học phí thực. > Yêu cầu về hồ sơ cầ...\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 Result 3\n",
            "Title: Điều 3\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.5306\n",
            "Vector Score: 0.3145\n",
            "Text Score: 0.7468\n",
            "Content Preview: Điều 3: Quy định về hỗ trợ học phí\n",
            "Sinh viên được hỗ trợ học phí cho 2 học kỳ chính (học kỳ 1 và học kỳ 2) nếu SV có phát sinh học phí trong học kỳ đó.\n",
            "Sinh viên thuộc diện hỗ trợ học phí nếu cùng một...\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 Result 4\n",
            "Title: Điều 1\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.4276\n",
            "Vector Score: 0.1336\n",
            "Text Score: 0.7216\n",
            "Content Preview: Điều 1: Đối tượng và mức hỗ trợ học phí\n",
            "Hỗ trợ học phí theo quy định tại Điều 9 của Quyết định này cho các sinh viên thuộc các đối tượng:\n",
            "Đối tượng 1: Hỗ trợ 10% học phí cho sinh viên mồ côi cha hoặc ...\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 Result 5\n",
            "Title: Điều 3\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.3670\n",
            "Vector Score: 0.6112\n",
            "Text Score: 0.1228\n",
            "Content Preview: Điều 3: Quy định về miễn, giảm học phí\n",
            "Sinh viên thuộc diện được miễn, giảm học phí, hỗ trợ chi phí học tập mà cùng một lúc được hưởng nhiều chính sách hỗ trợ khác nhau thì chỉ được hưởng một chế độ ư...\n",
            "--------------------------------------------------\n",
            "\n",
            "Final Results: 10 documents found\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain.schema import Document\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "from LLM_Routing import LLMResponse\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Connect to MongoDB\n",
        "mongo_uri = userdata.get(\"mongodb\")\n",
        "client = MongoClient(mongo_uri)\n",
        "collection = client[\"HCMIU_Data\"][\"Data\"]\n",
        "\n",
        "# Load embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"keepitreal/vietnamese-sbert\",\n",
        "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "def get_available_document_types() -> List[str]:\n",
        "    \"\"\"Get list of unique document types in the database\"\"\"\n",
        "    return collection.distinct(\"document_type\")\n",
        "\n",
        "def normalize_scores(scores: List[float]) -> List[float]:\n",
        "    \"\"\"Normalize scores to range [0,1] using min-max normalization\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "    min_score = min(scores)\n",
        "    max_score = max(scores)\n",
        "    if max_score == min_score:\n",
        "        return [1.0] * len(scores)\n",
        "    return [(score - min_score) / (max_score - min_score) for score in scores]\n",
        "\n",
        "def get_filtered_document_ids(document_type: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    First step: Get all document IDs that match the specified document type\n",
        "\n",
        "    Args:\n",
        "        document_type (str): The document type to filter by\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: List of document IDs matching the document type\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Query to get all documents of the specified type\n",
        "        filter_pipeline = [\n",
        "            {\n",
        "                \"$match\": {\n",
        "                    \"document_type\": document_type\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"_id\": 1\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        filtered_docs = list(collection.aggregate(filter_pipeline))\n",
        "        doc_ids = [doc[\"_id\"] for doc in filtered_docs]\n",
        "\n",
        "        logger.info(f\"Found {len(doc_ids)} documents of type '{document_type}'\")\n",
        "        return doc_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Failed to filter documents by type: {e}\")\n",
        "        return []\n",
        "\n",
        "def hybrid_search_on_filtered_docs(query: str, document_type: str, alpha: float = 0.5) -> List[Document]:\n",
        "    try:\n",
        "        # Log available document types\n",
        "        available_types = get_available_document_types()\n",
        "        logger.info(f\"Available document types in database: {available_types}\")\n",
        "        logger.info(f\"Searching for document type: {document_type}\")\n",
        "\n",
        "        # Check if document type exists\n",
        "        doc_count = collection.count_documents({\"document_type\": document_type})\n",
        "        if doc_count == 0:\n",
        "            logger.warning(f\"⚠️ No documents found for type: {document_type}\")\n",
        "            return []\n",
        "\n",
        "        logger.info(f\"Found {doc_count} documents of type '{document_type}'\")\n",
        "\n",
        "        # Special handling for course_structure documents\n",
        "        if document_type == \"course_structure\":\n",
        "            # Use only text search with title boost for course structure\n",
        "            bm25_pipeline = [\n",
        "                {\n",
        "                    \"$search\": {\n",
        "                        \"index\": \"text\",\n",
        "                        \"compound\": {\n",
        "                            \"must\": [\n",
        "                                {\n",
        "                                    \"text\": {\n",
        "                                        \"query\": query,\n",
        "                                        \"path\": \"title\",\n",
        "                                        \"score\": {\"boost\": {\"value\": 2}}  # Boost title matches\n",
        "                                    }\n",
        "                                }\n",
        "                            ],\n",
        "                            \"should\": [\n",
        "                                {\n",
        "                                    \"text\": {\n",
        "                                        \"query\": query,\n",
        "                                        \"path\": \"content\"\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"textScore\": { \"$meta\": \"searchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            bm25_results = list(collection.aggregate(bm25_pipeline))\n",
        "            logger.info(f\"Text search found {len(bm25_results)} results for course structure\")\n",
        "\n",
        "            if not bm25_results:\n",
        "                logger.warning(f\"⚠️ No relevant content found in course structure documents\")\n",
        "                return []\n",
        "\n",
        "            # Process results\n",
        "            docs = []\n",
        "            for doc in bm25_results:\n",
        "                doc = Document(\n",
        "                    page_content=doc.get(\"content\", \"\"),\n",
        "                    metadata={\n",
        "                        \"title\": doc.get(\"title\"),\n",
        "                        \"document_type\": doc.get(\"document_type\"),\n",
        "                        \"score\": doc.get(\"textScore\", 0),\n",
        "                        \"text_score\": doc.get(\"textScore\", 0),\n",
        "                        \"vector_score\": 0.0  # No vector score for course structure\n",
        "                    }\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "            # Sort by text score\n",
        "            docs.sort(key=lambda x: x.metadata.get(\"score\", 0), reverse=True)\n",
        "\n",
        "        else:\n",
        "            # Regular hybrid search for other document types\n",
        "            # Get query embedding\n",
        "            query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "            # BM25 Search\n",
        "            bm25_pipeline = [\n",
        "                {\n",
        "                    \"$search\": {\n",
        "                        \"index\": \"text\",\n",
        "                        \"text\": {\n",
        "                            \"query\": query,\n",
        "                            \"path\": [\"title\", \"content\"]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"textScore\": { \"$meta\": \"searchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            bm25_results = list(collection.aggregate(bm25_pipeline))\n",
        "            logger.info(f\"BM25 search found {len(bm25_results)} results\")\n",
        "\n",
        "            # Vector Search\n",
        "            vector_pipeline = [\n",
        "                {\n",
        "                    \"$vectorSearch\": {\n",
        "                        \"queryVector\": query_embedding,\n",
        "                        \"path\": \"embedding\",\n",
        "                        \"numCandidates\": 100,\n",
        "                        \"limit\": 50,\n",
        "                        \"index\": \"default\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"vectorScore\": { \"$meta\": \"vectorSearchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            vector_results = list(collection.aggregate(vector_pipeline))\n",
        "            logger.info(f\"Vector search found {len(vector_results)} results\")\n",
        "\n",
        "            if not bm25_results and not vector_results:\n",
        "                logger.warning(f\"⚠️ No relevant content found in documents of type: {document_type}\")\n",
        "                return []\n",
        "\n",
        "            # Combine and normalize scores\n",
        "            combined_results = {}\n",
        "\n",
        "            # Process BM25 results\n",
        "            if bm25_results:\n",
        "                bm25_scores = [doc.get(\"textScore\", 0) for doc in bm25_results]\n",
        "                normalized_bm25_scores = normalize_scores(bm25_scores)\n",
        "\n",
        "                for doc, norm_score in zip(bm25_results, normalized_bm25_scores):\n",
        "                    doc_id = doc.get(\"_id\")\n",
        "                    combined_results[doc_id] = {\n",
        "                        \"title\": doc.get(\"title\"),\n",
        "                        \"content\": doc.get(\"content\"),\n",
        "                        \"document_type\": doc.get(\"document_type\"),\n",
        "                        \"textScore\": norm_score,\n",
        "                        \"vectorScore\": 0.0\n",
        "                    }\n",
        "\n",
        "            # Process Vector results\n",
        "            if vector_results:\n",
        "                vector_scores = [doc.get(\"vectorScore\", 0) for doc in vector_results]\n",
        "                normalized_vector_scores = normalize_scores(vector_scores)\n",
        "\n",
        "                for doc, norm_score in zip(vector_results, normalized_vector_scores):\n",
        "                    doc_id = doc.get(\"_id\")\n",
        "                    if doc_id in combined_results:\n",
        "                        combined_results[doc_id][\"vectorScore\"] = norm_score\n",
        "                    else:\n",
        "                        combined_results[doc_id] = {\n",
        "                            \"title\": doc.get(\"title\"),\n",
        "                            \"content\": doc.get(\"content\"),\n",
        "                            \"document_type\": doc.get(\"document_type\"),\n",
        "                            \"textScore\": 0.0,\n",
        "                            \"vectorScore\": norm_score\n",
        "                        }\n",
        "\n",
        "            # Calculate final scores\n",
        "            docs = []\n",
        "            for doc_id, result in combined_results.items():\n",
        "                vector_score = result[\"vectorScore\"]\n",
        "                text_score = result[\"textScore\"]\n",
        "\n",
        "                combined_score = (alpha * vector_score) + ((1 - alpha) * text_score)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=result[\"content\"],\n",
        "                    metadata={\n",
        "                        \"title\": result[\"title\"],\n",
        "                        \"document_type\": result[\"document_type\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"vector_score\": vector_score,\n",
        "                        \"text_score\": text_score\n",
        "                    }\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "            # Sort by combined score\n",
        "            docs.sort(key=lambda x: x.metadata.get(\"score\", 0), reverse=True)\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nSearching for '{query}' in document type: {document_type}\")\n",
        "        print(f\"Total documents of this type: {doc_count}\")\n",
        "        print(f\"Found {len(docs)} relevant documents\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for i, doc in enumerate(docs[:5], 1):\n",
        "            print(f\"\\n📄 Result {i}\")\n",
        "            print(\"Title:\", doc.metadata.get(\"title\"))\n",
        "            print(\"Document Type:\", doc.metadata.get(\"document_type\"))\n",
        "            print(\"Combined Score:\", f\"{doc.metadata.get('score'):.4f}\")\n",
        "            print(\"Vector Score:\", f\"{doc.metadata.get('vector_score'):.4f}\")\n",
        "            print(\"Text Score:\", f\"{doc.metadata.get('text_score'):.4f}\")\n",
        "            print(\"Content Preview:\", doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        return docs[:10]\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Hybrid search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_documents(query: str, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Main search function that first filters by document type, then applies hybrid search.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query\n",
        "        alpha (float): Weight for vector search (0-1, where 1 = only vector, 0 = only BM25)\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Ranked search results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Get document type from LLM\n",
        "        llm_response = LLMResponse(userdata.get(\"OpenAI\"))\n",
        "        document_type = llm_response.get_function_call(query)\n",
        "\n",
        "        if not document_type:\n",
        "            logger.warning(\"⚠️ Could not determine document type from query\")\n",
        "            logger.info(\"Available document types:\")\n",
        "            for doc_type in get_available_document_types():\n",
        "                logger.info(f\"  - {doc_type}\")\n",
        "            return []\n",
        "\n",
        "        # Log the document type we're searching for\n",
        "        logger.info(f\"LLM determined document type: {document_type}\")\n",
        "\n",
        "        # Step 2: Perform hybrid search on filtered documents\n",
        "        return hybrid_search_on_filtered_docs(query, document_type, alpha)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Search process failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_documents_with_type(query: str, document_type: str, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Alternative function to search with explicit document type (bypassing LLM routing)\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query\n",
        "        document_type (str): Explicit document type to search within\n",
        "        alpha (float): Weight for vector search (0-1)\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Ranked search results\n",
        "    \"\"\"\n",
        "    logger.info(f\"Searching with explicit document type: {document_type}\")\n",
        "    return hybrid_search_on_filtered_docs(query, document_type, alpha)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"Sinh viên có anh chị em ruột đang theo học được giảm bao nhiêu học phí ?\"\n",
        "\n",
        "    # Method 1: Let LLM determine document type\n",
        "    results = search_documents(query, alpha=0.5)\n",
        "\n",
        "    # Method 2: Specify document type explicitly (if you know it)\n",
        "    # results = search_documents_with_type(query, \"academic_regulations\", alpha=0.5)\n",
        "\n",
        "    print(f\"\\nFinal Results: {len(results)} documents found\")"
      ]
    }
  ]
}