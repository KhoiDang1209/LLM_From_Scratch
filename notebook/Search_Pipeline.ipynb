{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain sentence-transformers torch transformers pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4StzHO38NY",
        "outputId": "e3397c0d-b8fb-41c4-d831-308ed09219e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dnspython, pymongo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dnspython-2.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pymongo-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface langchain_mongodb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey7LshiV5z3R",
        "outputId": "7848c900-2a78-4f96-fb23-85c67e61f9db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "Collecting langchain_mongodb\n",
            "  Downloading langchain_mongodb-0.6.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.60)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.52.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.31.4)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.8)\n",
            "Requirement already satisfied: langchain>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.25)\n",
            "Collecting lark<2.0.0,>=1.1.9 (from langchain_mongodb)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (2.0.2)\n",
            "Requirement already satisfied: pymongo>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo>=4.6.1->langchain_mongodb) (2.7.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3->langchain_mongodb) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.3->langchain_mongodb) (1.3.1)\n",
            "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_mongodb-0.6.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, langchain_huggingface, langchain_mongodb\n",
            "Successfully installed langchain_huggingface-0.2.0 langchain_mongodb-0.6.2 lark-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK9yaJZY3yD1",
        "outputId": "ded31f06-a6af-4eb2-dd52-a670f593fe67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: document_routing\n",
            "Arguments: {\n",
            "  \"document_type\": \"quy_dinh\"\n",
            "}\n",
            "\n",
            "Searching for 'Sinh viÃªn cÃ³ anh chá»‹ em ruá»™t Ä‘ang theo há»c Ä‘Æ°á»£c giáº£m bao nhiÃªu há»c phÃ­ ?' in document type: quy_dinh\n",
            "Total documents of this type: 363\n",
            "Found 29 relevant documents\n",
            "============================================================\n",
            "\n",
            "ğŸ“„ Result 1\n",
            "Title: Má»¥c 8\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 1.0000\n",
            "Vector Score: 1.0000\n",
            "Text Score: 1.0000\n",
            "Content Preview: Má»¥c 8: Sinh viÃªn cÃ³ anh chá»‹ em ruá»™t hiá»‡n Ä‘ang há»c táº¡i trÆ°á»ng Äáº¡i há»c Quá»‘c táº¿ (Ã¡p dá»¥ng ká»ƒ tá»« ngÆ°á»i thá»© hai).\n",
            "Má»©c miá»…n giáº£m: 10%\n",
            "YÃªu cáº§u: Báº£n sao cÃ´ng chá»©ng há»™ kháº©u thÆ°á»ng trÃº hoáº·c báº£n sao cÃ´ng chá»©ng gi...\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“„ Result 2\n",
            "Title: Má»¥c 7\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.5792\n",
            "Vector Score: 0.2349\n",
            "Text Score: 0.9236\n",
            "Content Preview: Má»¥c 7: Sinh viÃªn cÃ³ anh chá»‹ em ruá»™t hiá»‡n Ä‘ang há»c táº¡i trÆ°á»ng Äáº¡i há»c Quá»‘c táº¿ (Ã¡p dá»¥ng ká»ƒ tá»« ngÆ°á»i thá»© hai), (ChÃ­nh sÃ¡ch riÃªng cá»§a trÆ°á»ng ÄHOT). >\n",
            "Má»©c miá»…n giáº£m: 10% há»c phÃ­ thá»±c. > YÃªu cáº§u vá» há»“ sÆ¡ cáº§...\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“„ Result 3\n",
            "Title: Äiá»u 3\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.5306\n",
            "Vector Score: 0.3145\n",
            "Text Score: 0.7468\n",
            "Content Preview: Äiá»u 3: Quy Ä‘á»‹nh vá» há»— trá»£ há»c phÃ­\n",
            "Sinh viÃªn Ä‘Æ°á»£c há»— trá»£ há»c phÃ­ cho 2 há»c ká»³ chÃ­nh (há»c ká»³ 1 vÃ  há»c ká»³ 2) náº¿u SV cÃ³ phÃ¡t sinh há»c phÃ­ trong há»c ká»³ Ä‘Ã³.\n",
            "Sinh viÃªn thuá»™c diá»‡n há»— trá»£ há»c phÃ­ náº¿u cÃ¹ng má»™t...\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“„ Result 4\n",
            "Title: Äiá»u 1\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.4276\n",
            "Vector Score: 0.1336\n",
            "Text Score: 0.7216\n",
            "Content Preview: Äiá»u 1: Äá»‘i tÆ°á»£ng vÃ  má»©c há»— trá»£ há»c phÃ­\n",
            "Há»— trá»£ há»c phÃ­ theo quy Ä‘á»‹nh táº¡i Äiá»u 9 cá»§a Quyáº¿t Ä‘á»‹nh nÃ y cho cÃ¡c sinh viÃªn thuá»™c cÃ¡c Ä‘á»‘i tÆ°á»£ng:\n",
            "Äá»‘i tÆ°á»£ng 1: Há»— trá»£ 10% há»c phÃ­ cho sinh viÃªn má»“ cÃ´i cha hoáº·c ...\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“„ Result 5\n",
            "Title: Äiá»u 3\n",
            "Document Type: quy_dinh\n",
            "Combined Score: 0.3670\n",
            "Vector Score: 0.6112\n",
            "Text Score: 0.1228\n",
            "Content Preview: Äiá»u 3: Quy Ä‘á»‹nh vá» miá»…n, giáº£m há»c phÃ­\n",
            "Sinh viÃªn thuá»™c diá»‡n Ä‘Æ°á»£c miá»…n, giáº£m há»c phÃ­, há»— trá»£ chi phÃ­ há»c táº­p mÃ  cÃ¹ng má»™t lÃºc Ä‘Æ°á»£c hÆ°á»Ÿng nhiá»u chÃ­nh sÃ¡ch há»— trá»£ khÃ¡c nhau thÃ¬ chá»‰ Ä‘Æ°á»£c hÆ°á»Ÿng má»™t cháº¿ Ä‘á»™ Æ°...\n",
            "--------------------------------------------------\n",
            "\n",
            "Final Results: 10 documents found\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain.schema import Document\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "from LLM_Routing import LLMResponse\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Connect to MongoDB\n",
        "mongo_uri = userdata.get(\"mongodb\")\n",
        "client = MongoClient(mongo_uri)\n",
        "collection = client[\"HCMIU_Data\"][\"Data\"]\n",
        "\n",
        "# Load embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"keepitreal/vietnamese-sbert\",\n",
        "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "def get_available_document_types() -> List[str]:\n",
        "    \"\"\"Get list of unique document types in the database\"\"\"\n",
        "    return collection.distinct(\"document_type\")\n",
        "\n",
        "def normalize_scores(scores: List[float]) -> List[float]:\n",
        "    \"\"\"Normalize scores to range [0,1] using min-max normalization\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "    min_score = min(scores)\n",
        "    max_score = max(scores)\n",
        "    if max_score == min_score:\n",
        "        return [1.0] * len(scores)\n",
        "    return [(score - min_score) / (max_score - min_score) for score in scores]\n",
        "\n",
        "def get_filtered_document_ids(document_type: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    First step: Get all document IDs that match the specified document type\n",
        "\n",
        "    Args:\n",
        "        document_type (str): The document type to filter by\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: List of document IDs matching the document type\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Query to get all documents of the specified type\n",
        "        filter_pipeline = [\n",
        "            {\n",
        "                \"$match\": {\n",
        "                    \"document_type\": document_type\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"_id\": 1\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        filtered_docs = list(collection.aggregate(filter_pipeline))\n",
        "        doc_ids = [doc[\"_id\"] for doc in filtered_docs]\n",
        "\n",
        "        logger.info(f\"Found {len(doc_ids)} documents of type '{document_type}'\")\n",
        "        return doc_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ Failed to filter documents by type: {e}\")\n",
        "        return []\n",
        "\n",
        "def hybrid_search_on_filtered_docs(query: str, document_type: str, alpha: float = 0.5) -> List[Document]:\n",
        "    try:\n",
        "        # Log available document types\n",
        "        available_types = get_available_document_types()\n",
        "        logger.info(f\"Available document types in database: {available_types}\")\n",
        "        logger.info(f\"Searching for document type: {document_type}\")\n",
        "\n",
        "        # Check if document type exists\n",
        "        doc_count = collection.count_documents({\"document_type\": document_type})\n",
        "        if doc_count == 0:\n",
        "            logger.warning(f\"âš ï¸ No documents found for type: {document_type}\")\n",
        "            return []\n",
        "\n",
        "        logger.info(f\"Found {doc_count} documents of type '{document_type}'\")\n",
        "\n",
        "        # Special handling for course_structure documents\n",
        "        if document_type == \"course_structure\":\n",
        "            # Use only text search with title boost for course structure\n",
        "            bm25_pipeline = [\n",
        "                {\n",
        "                    \"$search\": {\n",
        "                        \"index\": \"text\",\n",
        "                        \"compound\": {\n",
        "                            \"must\": [\n",
        "                                {\n",
        "                                    \"text\": {\n",
        "                                        \"query\": query,\n",
        "                                        \"path\": \"title\",\n",
        "                                        \"score\": {\"boost\": {\"value\": 2}}  # Boost title matches\n",
        "                                    }\n",
        "                                }\n",
        "                            ],\n",
        "                            \"should\": [\n",
        "                                {\n",
        "                                    \"text\": {\n",
        "                                        \"query\": query,\n",
        "                                        \"path\": \"content\"\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"textScore\": { \"$meta\": \"searchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            bm25_results = list(collection.aggregate(bm25_pipeline))\n",
        "            logger.info(f\"Text search found {len(bm25_results)} results for course structure\")\n",
        "\n",
        "            if not bm25_results:\n",
        "                logger.warning(f\"âš ï¸ No relevant content found in course structure documents\")\n",
        "                return []\n",
        "\n",
        "            # Process results\n",
        "            docs = []\n",
        "            for doc in bm25_results:\n",
        "                doc = Document(\n",
        "                    page_content=doc.get(\"content\", \"\"),\n",
        "                    metadata={\n",
        "                        \"title\": doc.get(\"title\"),\n",
        "                        \"document_type\": doc.get(\"document_type\"),\n",
        "                        \"score\": doc.get(\"textScore\", 0),\n",
        "                        \"text_score\": doc.get(\"textScore\", 0),\n",
        "                        \"vector_score\": 0.0  # No vector score for course structure\n",
        "                    }\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "            # Sort by text score\n",
        "            docs.sort(key=lambda x: x.metadata.get(\"score\", 0), reverse=True)\n",
        "\n",
        "        else:\n",
        "            # Regular hybrid search for other document types\n",
        "            # Get query embedding\n",
        "            query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "            # BM25 Search\n",
        "            bm25_pipeline = [\n",
        "                {\n",
        "                    \"$search\": {\n",
        "                        \"index\": \"text\",\n",
        "                        \"text\": {\n",
        "                            \"query\": query,\n",
        "                            \"path\": [\"title\", \"content\"]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"textScore\": { \"$meta\": \"searchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            bm25_results = list(collection.aggregate(bm25_pipeline))\n",
        "            logger.info(f\"BM25 search found {len(bm25_results)} results\")\n",
        "\n",
        "            # Vector Search\n",
        "            vector_pipeline = [\n",
        "                {\n",
        "                    \"$vectorSearch\": {\n",
        "                        \"queryVector\": query_embedding,\n",
        "                        \"path\": \"embedding\",\n",
        "                        \"numCandidates\": 100,\n",
        "                        \"limit\": 50,\n",
        "                        \"index\": \"default\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$match\": {\n",
        "                        \"document_type\": document_type\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$project\": {\n",
        "                        \"title\": 1,\n",
        "                        \"content\": 1,\n",
        "                        \"document_type\": 1,\n",
        "                        \"vectorScore\": { \"$meta\": \"vectorSearchScore\" }\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"$limit\": 20\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            vector_results = list(collection.aggregate(vector_pipeline))\n",
        "            logger.info(f\"Vector search found {len(vector_results)} results\")\n",
        "\n",
        "            if not bm25_results and not vector_results:\n",
        "                logger.warning(f\"âš ï¸ No relevant content found in documents of type: {document_type}\")\n",
        "                return []\n",
        "\n",
        "            # Combine and normalize scores\n",
        "            combined_results = {}\n",
        "\n",
        "            # Process BM25 results\n",
        "            if bm25_results:\n",
        "                bm25_scores = [doc.get(\"textScore\", 0) for doc in bm25_results]\n",
        "                normalized_bm25_scores = normalize_scores(bm25_scores)\n",
        "\n",
        "                for doc, norm_score in zip(bm25_results, normalized_bm25_scores):\n",
        "                    doc_id = doc.get(\"_id\")\n",
        "                    combined_results[doc_id] = {\n",
        "                        \"title\": doc.get(\"title\"),\n",
        "                        \"content\": doc.get(\"content\"),\n",
        "                        \"document_type\": doc.get(\"document_type\"),\n",
        "                        \"textScore\": norm_score,\n",
        "                        \"vectorScore\": 0.0\n",
        "                    }\n",
        "\n",
        "            # Process Vector results\n",
        "            if vector_results:\n",
        "                vector_scores = [doc.get(\"vectorScore\", 0) for doc in vector_results]\n",
        "                normalized_vector_scores = normalize_scores(vector_scores)\n",
        "\n",
        "                for doc, norm_score in zip(vector_results, normalized_vector_scores):\n",
        "                    doc_id = doc.get(\"_id\")\n",
        "                    if doc_id in combined_results:\n",
        "                        combined_results[doc_id][\"vectorScore\"] = norm_score\n",
        "                    else:\n",
        "                        combined_results[doc_id] = {\n",
        "                            \"title\": doc.get(\"title\"),\n",
        "                            \"content\": doc.get(\"content\"),\n",
        "                            \"document_type\": doc.get(\"document_type\"),\n",
        "                            \"textScore\": 0.0,\n",
        "                            \"vectorScore\": norm_score\n",
        "                        }\n",
        "\n",
        "            # Calculate final scores\n",
        "            docs = []\n",
        "            for doc_id, result in combined_results.items():\n",
        "                vector_score = result[\"vectorScore\"]\n",
        "                text_score = result[\"textScore\"]\n",
        "\n",
        "                combined_score = (alpha * vector_score) + ((1 - alpha) * text_score)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=result[\"content\"],\n",
        "                    metadata={\n",
        "                        \"title\": result[\"title\"],\n",
        "                        \"document_type\": result[\"document_type\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"vector_score\": vector_score,\n",
        "                        \"text_score\": text_score\n",
        "                    }\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "            # Sort by combined score\n",
        "            docs.sort(key=lambda x: x.metadata.get(\"score\", 0), reverse=True)\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nSearching for '{query}' in document type: {document_type}\")\n",
        "        print(f\"Total documents of this type: {doc_count}\")\n",
        "        print(f\"Found {len(docs)} relevant documents\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for i, doc in enumerate(docs[:5], 1):\n",
        "            print(f\"\\nğŸ“„ Result {i}\")\n",
        "            print(\"Title:\", doc.metadata.get(\"title\"))\n",
        "            print(\"Document Type:\", doc.metadata.get(\"document_type\"))\n",
        "            print(\"Combined Score:\", f\"{doc.metadata.get('score'):.4f}\")\n",
        "            print(\"Vector Score:\", f\"{doc.metadata.get('vector_score'):.4f}\")\n",
        "            print(\"Text Score:\", f\"{doc.metadata.get('text_score'):.4f}\")\n",
        "            print(\"Content Preview:\", doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        return docs[:10]\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ Hybrid search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_documents(query: str, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Main search function that first filters by document type, then applies hybrid search.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query\n",
        "        alpha (float): Weight for vector search (0-1, where 1 = only vector, 0 = only BM25)\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Ranked search results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Get document type from LLM\n",
        "        llm_response = LLMResponse(userdata.get(\"OpenAI\"))\n",
        "        document_type = llm_response.get_function_call(query)\n",
        "\n",
        "        if not document_type:\n",
        "            logger.warning(\"âš ï¸ Could not determine document type from query\")\n",
        "            logger.info(\"Available document types:\")\n",
        "            for doc_type in get_available_document_types():\n",
        "                logger.info(f\"  - {doc_type}\")\n",
        "            return []\n",
        "\n",
        "        # Log the document type we're searching for\n",
        "        logger.info(f\"LLM determined document type: {document_type}\")\n",
        "\n",
        "        # Step 2: Perform hybrid search on filtered documents\n",
        "        return hybrid_search_on_filtered_docs(query, document_type, alpha)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ Search process failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_documents_with_type(query: str, document_type: str, alpha: float = 0.5) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Alternative function to search with explicit document type (bypassing LLM routing)\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query\n",
        "        document_type (str): Explicit document type to search within\n",
        "        alpha (float): Weight for vector search (0-1)\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Ranked search results\n",
        "    \"\"\"\n",
        "    logger.info(f\"Searching with explicit document type: {document_type}\")\n",
        "    return hybrid_search_on_filtered_docs(query, document_type, alpha)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"Sinh viÃªn cÃ³ anh chá»‹ em ruá»™t Ä‘ang theo há»c Ä‘Æ°á»£c giáº£m bao nhiÃªu há»c phÃ­ ?\"\n",
        "\n",
        "    # Method 1: Let LLM determine document type\n",
        "    results = search_documents(query, alpha=0.5)\n",
        "\n",
        "    # Method 2: Specify document type explicitly (if you know it)\n",
        "    # results = search_documents_with_type(query, \"academic_regulations\", alpha=0.5)\n",
        "\n",
        "    print(f\"\\nFinal Results: {len(results)} documents found\")"
      ]
    }
  ]
}